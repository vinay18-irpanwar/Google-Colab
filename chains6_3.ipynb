{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlrthvyxLU96G8k5BIf234",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinay18-irpanwar/Google-Colab/blob/main/chains6_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z53rFf6nygG6"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_inputs=\"chaining in Langchain\""
      ],
      "metadata": {
        "id": "BFbzAD6Zy8MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##chaining without langchain with plain python"
      ],
      "metadata": {
        "id": "yeDgiuJM06a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client=genai.Client(api_key=userdata.get('lang'))\n",
        "\n",
        "prompt=f\"generate a short report on {user_inputs}\"\n",
        "response=client.models.generate_content(model=\"gemini-2.5-flash\",contents=prompt,)\n",
        "\n",
        "report=response.text"
      ],
      "metadata": {
        "id": "O_OTLJhMzIJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2=f\"generate a summary on {report}\"\n",
        "response1=client.models.generate_content(model=\"gemini-2.5-flash\",contents=prompt2,)\n",
        "\n",
        "summary=response1.text\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SvtJIGq0aMb",
        "outputId": "905a7843-149b-41c3-f60c-5cdd81deea36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chaining in Langchain is a fundamental concept for building sophisticated applications with Large Language Models (LLMs). It involves combining multiple components—such as LLMs, prompt templates, output parsers, tools, memory, retrievers, and even other chains—into a single, cohesive workflow.\n",
            "\n",
            "This approach is crucial because real-world tasks often require more than a single LLM call, necessitating steps like preprocessing input, interacting with external data, applying conditional logic, remembering past interactions, and post-processing LLM outputs. Chains provide the programmatic structure to link these disparate steps seamlessly.\n",
            "\n",
            "Langchain offers various built-in chain types, from the basic `LLMChain` to `SequentialChain` for ordered execution, `RetrievalQAChain` for Retrieval Augmented Generation (RAG), and `RouterChain` for dynamic behavior based on input.\n",
            "\n",
            "The benefits of chaining include modularity, enhanced capabilities through external integrations, improved readability and maintainability for complex applications, and serving as the foundational backbone for building intelligent \"Agents.\" Ultimately, chaining empowers developers to create robust, context-aware LLM applications capable of multi-step reasoning and interaction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##chaining with langchain (pipline)"
      ],
      "metadata": {
        "id": "lgODoh4r1CFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "GrA2CHPu1HFD",
        "outputId": "61b414c1-a1f5-45e2-847b-6e8027daab3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-3.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.40)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2,>=1.2 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.9.0 langchain_google_genai-2.1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c51ab3db5a854874a7a13ebf28acc62e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "PoBH2hoR4lxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLM=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=userdata.get('lang'))\n",
        "\n",
        "template=PromptTemplate(template=\"generate a 10 line report on {user_inputs}\",input=[\"user_inputs\"])\n",
        "prompts=template.format(user_inputs=\"prompt\")\n",
        "result=LLM.invoke(prompts)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "jND7W-YC5Bh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e61be1-bc1a-480b-c10f-d99b08f8e614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Report on Prompt**\n",
            "\n",
            "1.  A prompt is an initial instruction, query, or stimulus provided to a system or individual.\n",
            "2.  Its primary function is to elicit a specific response, action, or creative output.\n",
            "3.  Prompts serve as the foundational input for large language models (LLMs) and generative AI.\n",
            "4.  They guide the AI in generating text, images, code, or other media according to user intent.\n",
            "5.  Effective prompts are typically clear, concise, and provide sufficient context.\n",
            "6.  The quality of a prompt directly impacts the relevance and accuracy of the generated result.\n",
            "7.  Beyond AI, prompts are crucial in programming, creative writing, user interfaces, and education.\n",
            "8.  \"Prompt engineering\" is the skill of crafting optimal prompts for desired outcomes.\n",
            "9.  Prompts can range from simple keywords to complex, multi-layered instructions.\n",
            "10. Ultimately, prompts act as the essential bridge between human intention and system execution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with pipline operator\n"
      ],
      "metadata": {
        "id": "1p_fYkvpEa5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=userdata.get('lang'))\n",
        "\n",
        "template=PromptTemplate(template=\"generate a 10 line report on {user_inputs}\",input=[\"user_inputs\"])\n",
        "\n",
        "chain=template | LLM\n",
        "\n",
        "result3=(chain.invoke({\"user_inputs\":\"prompt engineering\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a-gW9TaEYsl",
        "outputId": "b2953c2f-d0a7-4c05-8645-6d2a0d8b82df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Report on Prompt Engineering**\n",
            "\n",
            "1.  Prompt engineering is the discipline of crafting effective inputs for AI models.\n",
            "2.  Its primary goal is to guide generative AI, especially LLMs, to produce desired and accurate outputs.\n",
            "3.  This involves carefully structuring instructions, context, examples, and constraints within the prompt.\n",
            "4.  Key techniques include zero-shot, few-shot, and chain-of-thought prompting.\n",
            "5.  Strategies like persona assignment, step-by-step instructions, and output formatting are also employed.\n",
            "6.  Effective prompt engineering significantly enhances AI accuracy, relevance, and reliability.\n",
            "7.  It reduces model \"hallucinations\" and optimizes performance for specific tasks.\n",
            "8.  This skill is crucial for leveraging the full potential of AI in various applications.\n",
            "9.  It requires a blend of understanding AI capabilities, creativity, and iterative refinement.\n",
            "10. Prompt engineering is an evolving and essential bridge between human intent and AI execution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  output_parsers\n",
        "it directly print content we dont need .content"
      ],
      "metadata": {
        "id": "jayrE7-lQwVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "summary_prompt=PromptTemplate(template=\"create a short summary on {result3}\",inputs=[\"result3\"])\n",
        "\n",
        "\n",
        "summary_chaining=summary_prompt | LLM | StrOutputParser()\n",
        "\n",
        "r=summary_chaining.invoke({\"result3\":result3})\n",
        "print(r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C8gJ470JSb8",
        "outputId": "3b27ec49-3483-428e-a2dc-7fea10ab99d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt engineering is the critical discipline of crafting effective inputs for AI models, especially LLMs, to guide them toward desired, accurate, and reliable outputs. It involves carefully structuring instructions, context, examples, and constraints using techniques like zero-shot, few-shot, and chain-of-thought prompting, along with strategies like persona assignment and step-by-step instructions. This skill significantly enhances AI accuracy, reduces hallucinations, and optimizes performance, making it essential for leveraging AI's full potential and bridging human intent with AI execution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#reduced code\n",
        "instend of invoking tow times\n",
        "creating one big chain\n"
      ],
      "metadata": {
        "id": "pfFQ8knOS3l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=userdata.get('lang'))\n",
        "\n",
        "template=PromptTemplate(template=\"generate a 10 line report on {user_inputs}\",input=[\"user_inputs\"])\n",
        "summary_prompt=PromptTemplate(template=\"create a short summary on {result3}\",inputs=[\"result3\"])\n",
        "#add funny quote LLM\n",
        "funny_prompt=PromptTemplate(template=\"based on {funny} create a funny quote\",input=[\"funny\"])\n",
        "chain=template | LLM |StrOutputParser() | summary_prompt | LLM | StrOutputParser() | funny_prompt | LLM | StrOutputParser()\n",
        "#note :- inputparament must of first template\n",
        "#automaticaly LLM output  is given to LLM2\n",
        "result3=(chain.invoke({\"user_inputs\":\"prompt engineering\"}))\n",
        "print(result3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj4M72mITD3D",
        "outputId": "3afb9e16-48c7-44d5-be5b-eb111ccb1cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Prompt engineering: It's the delicate art of writing a 500-word essay to ask a supercomputer for a haiku, and still occasionally getting a recipe for banana bread.\"\n"
          ]
        }
      ]
    }
  ]
}